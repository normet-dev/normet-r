% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/nm_pipeline_do_all.r
\name{nm_do_all_unc}
\alias{nm_do_all_unc}
\title{Perform Normalisation with Uncertainty Estimation}
\usage{
nm_do_all_unc(
  df = NULL,
  value = "value",
  backend = "h2o",
  feature_names = NULL,
  variables_resample = NULL,
  split_method = "random",
  fraction = 0.75,
  model_config = NULL,
  n_samples = 300,
  n_models = 10,
  confidence_level = 0.95,
  seed = 7654321,
  n_cores = NULL,
  memory_save = FALSE,
  verbose = TRUE,
  weighted_method = "r2",
  cleanup_every = 5
)
}
\arguments{
\item{df}{The raw input data frame.}

\item{value}{The target variable name as a string.}

\item{backend}{The modeling backend to use for training. Default is 'h2o'.}

\item{feature_names}{The names of the features used for training and normalisation.}

\item{variables_resample}{The names of variables to be resampled during normalisation.}

\item{split_method}{The method for splitting data into training and testing sets.}

\item{fraction}{The proportion of the data to be used for training.}

\item{model_config}{A list containing configuration parameters for model training.}

\item{n_samples}{Number of times to sample the data for normalisation.}

\item{n_models}{Number of models to train for uncertainty estimation.}

\item{confidence_level}{The confidence level for uncertainty estimation.}

\item{seed}{A random seed for reproducibility.}

\item{n_cores}{Number of CPU cores to use for parallel processing.}

\item{memory_save}{Logical for memory-efficient normalisation.}

\item{verbose}{Should the function print progress messages and logs?}

\item{weighted_method}{Method for weighting models ("r2" or "rmse").}

\item{cleanup_every}{Integer; how often to clear H2O memory (only relevant if backend = "h2o").}
}
\value{
A list containing the normalised data with uncertainty and model statistics.
}
\description{
Trains multiple models, aggregates their normalised predictions,
and builds uncertainty bands with optional performance-based weighting.
}
